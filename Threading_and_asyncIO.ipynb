{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO63b1ejH0343J4YAc+KLrZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tailorfenil/BigDataLab_InteractiveScatterPlot_D3/blob/master/Threading_and_asyncIO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from urllib.parse import urlparse as urlparser\n",
        "from typing import Set  # Import Set from typing module"
      ],
      "metadata": {
        "id": "eP3yp6iM_rlX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test \"webs\" as graphs\n",
        "test_graph_1 = {\n",
        "    \"http://site.com/a\": [\"http://site.com/b\", \"http://site.com/c\",\"http://site.com/d\"],\n",
        "    \"http://site.com/b\": [\"http://site.com/d\"],\n",
        "    \"http://site.com/c\": [],\n",
        "    \"http://site.com/d\": []\n",
        "}\n",
        "\n",
        "test_graph_2 = {\n",
        "    \"http://site.com/a\": [\"http://site.com/b\", \"http://othersite.com/e\"],\n",
        "    \"http://site.com/b\": [\"http://site.com/c\"],\n",
        "    \"http://site.com/c\": [],\n",
        "    \"http://othersite.com/e\": [\"http://othersite.com/f\"],\n",
        "    \"http://othersite.com/f\": []\n",
        "}\n"
      ],
      "metadata": {
        "id": "4x2WtTb8-f7g"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class htmlparser:\n",
        "\n",
        "  def __init__(self,web_crawler_map):\n",
        "    self.web_crawler_map = web_crawler_map\n",
        "\n",
        "  def parse(self,url:str) -> list[str]:\n",
        "    return self.web_crawler_map.get(url,[])"
      ],
      "metadata": {
        "id": "sy9tth1_9Yic"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wthiOb_33Dt"
      },
      "outputs": [],
      "source": [
        "\n",
        "class sequential_crawl:\n",
        "  def __init__(self,url:str,htmlparser):\n",
        "    self.url = url\n",
        "    self.htmlparser = htmlparser\n",
        "    self.visited = set()\n",
        "    self.hostname = urlparser(url).hostname\n",
        "\n",
        "  def crawl(self,url:str):\n",
        "    if url in self.visited or urlparser(url).hostname != self.hostname:\n",
        "      return\n",
        "    self.visited.add(url)\n",
        "    print(f\"Crawling {url}\")\n",
        "    for child in self.htmlparser.parse(url):\n",
        "      self.crawl(child)\n",
        "\n",
        "  def run(self,start_url:str) -> Set[str]:\n",
        "    self.crawl(start_url)\n",
        "    return self.visited\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st_time = time.time()\n",
        "crawl = sequential_crawl(\"http://site.com/a\",htmlparser(test_graph_1))\n",
        "visited = crawl.run(\"http://site.com/a\")\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrsy_SN8-8SN",
        "outputId": "9ab3686a-2a2b-4b2e-df84-307cfca8d109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling http://site.com/a\n",
            "Crawling http://site.com/b\n",
            "Crawling http://site.com/d\n",
            "Crawling http://site.com/c\n",
            "Time taken: 0.00031638145446777344 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sequential_crawl completes"
      ],
      "metadata": {
        "id": "2VfkbTBf_CdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "class crawl_threading:\n",
        "  def __init__(self,url:str,htmlparser) -> None:\n",
        "    self.url=url\n",
        "    self.visited=set()\n",
        "    self.htmlparser = htmlparser\n",
        "    self.hostname = urlparser(url).hostname\n",
        "\n",
        "  def crawl(self,url:str):\n",
        "    if url in self.visited or urlparser(url).hostname != self.hostname:\n",
        "      return\n",
        "    self.visited.add(url)\n",
        "    print(f\"Crawling {url}\")\n",
        "\n",
        "    threads=[]\n",
        "    for child in self.htmlparser.parse(url):\n",
        "      thread = threading.Thread(target=self.crawl,args=(child,)) # why , at end?\n",
        "      thread.start()\n",
        "      threads.append(thread)\n",
        "\n",
        "    for thread in threads:\n",
        "      thread.join()\n"
      ],
      "metadata": {
        "id": "_9I_jgkEA7hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crawlthreads = crawl_threading(\"http://site.com/a\",htmlparser(test_graph_1))\n",
        "st_time = time.time()\n",
        "visite= crawlthreads.crawl(\"http://site.com/a\")\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(visited)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpsGVEadBhBH",
        "outputId": "d31dde2c-9101-4413-8d8e-2c53e01a897a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling http://site.com/a\n",
            "Crawling http://site.com/b\n",
            "Crawling http://site.com/d\n",
            "Crawling http://site.com/c\n",
            "Time taken: 0.0030410289764404297 seconds\n",
            "{'http://site.com/a', 'http://site.com/d', 'http://site.com/b', 'http://site.com/c'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "crawl_threading completes"
      ],
      "metadata": {
        "id": "LptdjPs3_Eoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class crawl_thread_safe:\n",
        "  def __init__(self,url:str,htmlparser) -> None:\n",
        "    self.url=url\n",
        "    self.visited=set()\n",
        "    self.htmlparser = htmlparser\n",
        "    self.hostname = urlparser(url).hostname\n",
        "    self.lock = threading.Lock()\n",
        "\n",
        "  def crawl(self,url:str):\n",
        "    with self.lock:\n",
        "      if url in self.visited or urlparser(url).hostname != self.hostname:\n",
        "        return\n",
        "      self.visited.add(url)\n",
        "      print(f\"Crawling {url}\")\n",
        "\n",
        "    threads=[]\n",
        "\n",
        "    for child in self.htmlparser.parse(url):\n",
        "      thread = threading.Thread(target=self.crawl,args=(child,))\n",
        "      thread.start()\n",
        "      threads.append(thread)\n",
        "\n",
        "    for thread in threads:\n",
        "      thread.join()\n",
        "\n",
        "  def run(self,start_url:str) -> Set[str]:\n",
        "    self.crawl(start_url)\n",
        "    return self.visited"
      ],
      "metadata": {
        "id": "ekz2jzTUCvDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st_time = time.time()\n",
        "crawlthreadsafe = crawl_thread_safe(\"http://site.com/a\",htmlparser(test_graph_1))\n",
        "visited = crawlthreadsafe.run(\"http://site.com/a\")\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(visited)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaHNLU7eDLos",
        "outputId": "3f618441-6f49-4638-feb2-5d10cfc7dbb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling http://site.com/a\n",
            "Crawling http://site.com/b\n",
            "Crawling http://site.com/d\n",
            "Crawling http://site.com/c\n",
            "Time taken: 0.0024394989013671875 seconds\n",
            "{'http://site.com/a', 'http://site.com/d', 'http://site.com/b', 'http://site.com/c'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "crawl_thread_safe completes"
      ],
      "metadata": {
        "id": "GUE86BRc_InM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class crawl_thread_safe_concurrrency_limit:\n",
        "  def __init__(self,url:str,htmlparser,max_threads:int=10) -> None:\n",
        "    self.url=url\n",
        "    self.visited=set()\n",
        "    self.htmlparser = htmlparser\n",
        "    self.hostname = urlparser(url).hostname\n",
        "    self.lock= threading.Lock()\n",
        "    self.semaphore = threading.Semaphore(max_threads)\n",
        "\n",
        "  def crawl(self,url:str):\n",
        "    with self.semaphore:\n",
        "      with self.lock:\n",
        "        if url in self.visited or urlparser(url).hostname != self.hostname:\n",
        "          return\n",
        "        self.visited.add(url)\n",
        "        print(f\"Crawling {url}\")\n",
        "\n",
        "      threads = []\n",
        "      for child in self.htmlparser.parse(url):\n",
        "        thread = threading.Thread(target=self.crawl,args=(child,))\n",
        "        thread.start()\n",
        "        threads.append(thread)\n",
        "\n",
        "      for thread in threads:\n",
        "        thread.join()\n"
      ],
      "metadata": {
        "id": "5t9srk5WExcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crawl_concurrency_limit = crawl_thread_safe_concurrrency_limit(\"http://site.com/a\",htmlparser(test_graph_1))\n",
        "st_time = time.time()\n",
        "visited = crawl_concurrency_limit.crawl(\"http://site.com/a\")\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(visited)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFzcIBzsFLhB",
        "outputId": "e54a0481-e983-4b2b-d080-7e163dce710a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling http://site.com/a\n",
            "Crawling http://site.com/b\n",
            "Crawling http://site.com/d\n",
            "Crawling http://site.com/c\n",
            "Time taken: 0.0018458366394042969 seconds\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "crawl_thread_safe_concurrrency_limit completes"
      ],
      "metadata": {
        "id": "lAeZpD07_LVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, wait\n",
        "class crawlthreadingwithThreadPool:\n",
        "  def __init__(self,url,htmlparser,max_threads:int=10):\n",
        "    self.url=url\n",
        "    self.htmlparser= htmlparser\n",
        "    self.visited=set()\n",
        "    self.hostname = urlparser(url).hostname\n",
        "    self.lock = threading.Lock()\n",
        "    self.executor = ThreadPoolExecutor(max_workers=max_threads)\n",
        "    self.futures = []\n",
        "\n",
        "  def crawl(self,url):\n",
        "    with self.lock:\n",
        "      if url in self.visited or urlparser(url).hostname != self.hostname:\n",
        "        return\n",
        "      self.visited.add(url)\n",
        "      print(f\"Crawling {url}\")\n",
        "\n",
        "    for child in self.htmlparser.parse(url):\n",
        "      future = self.executor.submit(self.crawl,child)\n",
        "      with self.lock:\n",
        "        self.futures.append(future)\n",
        "\n",
        "\n",
        "  def run(self,start_url):\n",
        "    future = self.executor.submit(self.crawl,start_url)\n",
        "    with self.lock:\n",
        "      self.futures.append(future)\n",
        "    wait(self.futures)\n",
        "    self.executor.shutdown()\n",
        "    return self.visited"
      ],
      "metadata": {
        "id": "Refs60NvKkW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crawlThreadPool= crawlthreadingwithThreadPool(\"http://site.com/a\",htmlparser(test_graph_1))\n",
        "st_time = time.time()\n",
        "visited = crawlThreadPool.run(\"http://site.com/a\")\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(visited)\n",
        "\n",
        "'''\n",
        "But your code does this:\n",
        "\n",
        "Starts with one future for crawl(a)\n",
        "That task submits more futures from inside threads\n",
        "Those new futures are added to self.futures later\n",
        "But wait(...) is already running and doesn’t know about them\n",
        "So wait(self.futures) can return before the newly submitted tasks are finished, and the crawler may shut down prematurely.\n",
        "\n",
        "ou're submitting future tasks asynchronously, and the list of self.futures keeps growing after wait() starts.\n",
        "But wait(self.futures) only waits on what’s already in that list — it doesn't magically re-evaluate it.\n",
        "\n",
        " When You DO Need a Condition\n",
        "You need a Condition when:\n",
        "\n",
        "New work is discovered dynamically (like recursive crawling)\n",
        "You don’t know how many total tasks there will be up front\n",
        "You want to wait for all dynamic tasks to be “done”, not just submitted\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "_uL6KbKyMmqy",
        "outputId": "cbd97c9a-7a6c-4196-d678-de71d73e1348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling http://site.com/a\n",
            "Crawling http://site.com/b\n",
            "Crawling http://site.com/c\n",
            "Crawling http://site.com/d\n",
            "Time taken: 0.0026743412017822266 seconds\n",
            "{'http://site.com/a', 'http://site.com/d', 'http://site.com/b', 'http://site.com/c'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nBut your code does this:\\n\\nStarts with one future for crawl(a)\\nThat task submits more futures from inside threads\\nThose new futures are added to self.futures later\\nBut wait(...) is already running and doesn’t know about them\\nSo wait(self.futures) can return before the newly submitted tasks are finished, and the crawler may shut down prematurely.\\n\\nou're submitting future tasks asynchronously, and the list of self.futures keeps growing after wait() starts.\\nBut wait(self.futures) only waits on what’s already in that list — it doesn't magically re-evaluate it.\\n\\n When You DO Need a Condition\\nYou need a Condition when:\\n\\nNew work is discovered dynamically (like recursive crawling)\\nYou don’t know how many total tasks there will be up front\\nYou want to wait for all dynamic tasks to be “done”, not just submitted\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 400
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "crawlthreadingwithThreadPool completes"
      ],
      "metadata": {
        "id": "r4nTqjXg_PAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExecutorBasedCrawler:\n",
        "    def __init__(self, start_url: str, htmlParser, max_workers: int = 10):\n",
        "        self.start_url = start_url\n",
        "        self.htmlParser = htmlParser\n",
        "        self.hostname = urlparser(start_url).hostname\n",
        "        self.visited = set()\n",
        "        self.lock = threading.Lock()\n",
        "        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n",
        "        self.active_tasks = 0\n",
        "        self.condition = threading.Condition()\n",
        "\n",
        "    def crawl_url(self, url: str):\n",
        "        with self.lock:\n",
        "            if url in self.visited or urlparser(url).hostname != self.hostname:\n",
        "                return\n",
        "            self.visited.add(url)\n",
        "\n",
        "        with self.condition:\n",
        "            self.active_tasks += 1\n",
        "\n",
        "        print(f\"Crawling {url}\")\n",
        "        try:\n",
        "            children = self.htmlParser.parse(url)\n",
        "            print(children)\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "            with self.condition:\n",
        "                self.active_tasks -= 1\n",
        "                self.condition.notify_all()\n",
        "            return\n",
        "\n",
        "        for child in children:\n",
        "            self.executor.submit(self.crawl_url, child)\n",
        "\n",
        "        with self.condition:\n",
        "            self.active_tasks -= 1\n",
        "            self.condition.notify_all()\n",
        "\n",
        "    def run(self) -> Set[str]:\n",
        "        self.executor.submit(self.crawl_url, self.start_url)\n",
        "        with self.condition:\n",
        "            while self.active_tasks > 0:\n",
        "                self.condition.wait()\n",
        "        self.executor.shutdown()\n",
        "        return self.visited\n"
      ],
      "metadata": {
        "id": "8wYCqd-9XH5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exec_condition_crawler= ExecutorBasedCrawler(\"http://site.com/a\",htmlparser(test_graph_1))\n",
        "st_time = time.time()\n",
        "visited = exec_condition_crawler.run()\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(visited)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCNLFlv0YxI1",
        "outputId": "39105f79-7070-45a2-c17b-bcb6cf36d5c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling http://site.com/a\n",
            "['http://site.com/b', 'http://site.com/c', 'http://site.com/d']\n",
            "Crawling http://site.com/b\n",
            "['http://site.com/d']\n",
            "Crawling http://site.com/c\n",
            "[]\n",
            "Crawling http://site.com/d\n",
            "[]\n",
            "Time taken: 0.002387523651123047 seconds\n",
            "{'http://site.com/a', 'http://site.com/d', 'http://site.com/b', 'http://site.com/c'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ExecutorBasedCrawler completes"
      ],
      "metadata": {
        "id": "FrmXWHu5_S35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "class HybridAsyncCrawler:\n",
        "    def __init__(self):\n",
        "        self.visited = set()\n",
        "        self.lock = asyncio.Lock()\n",
        "        self.queue = asyncio.Queue()\n",
        "\n",
        "    async def downloadUrls(self, url, htmlParser, hostname, loop):\n",
        "        # Run sync getUrls in a background thread (non-blocking for asyncio)\n",
        "        next_urls = await loop.run_in_executor(None, htmlParser.parse, url)\n",
        "\n",
        "        for u in next_urls:\n",
        "            if urlparse(u).hostname == hostname:\n",
        "                async with self.lock:\n",
        "                    if u not in self.visited:\n",
        "                        self.visited.add(u)\n",
        "                        await self.queue.put(u)\n",
        "\n",
        "    async def crawl(self, startUrl: str, htmlParser):\n",
        "        loop = asyncio.get_event_loop()\n",
        "        hostname = urlparse(startUrl).hostname\n",
        "        self.visited = {startUrl}\n",
        "        await self.queue.put(startUrl)\n",
        "\n",
        "        tasks = []\n",
        "\n",
        "        while not self.queue.empty() or tasks:\n",
        "            while not self.queue.empty():\n",
        "                url = await self.queue.get()\n",
        "                task = asyncio.create_task(\n",
        "                    self.downloadUrls(url, htmlParser, hostname, loop)\n",
        "                )\n",
        "                tasks.append(task)\n",
        "\n",
        "            # Clean up finished tasks\n",
        "            tasks = [t for t in tasks if not t.done()]\n",
        "            await asyncio.sleep(0.01)  # Give other coroutines a chance to run\n",
        "\n",
        "        return list(self.visited)"
      ],
      "metadata": {
        "id": "98jcpXPD7sV7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = htmlparser(test_graph_1)\n",
        "crawler = HybridAsyncCrawler()\n",
        "st_time = time.time()\n",
        "# result = asyncio.run(crawler.crawl(\"http://site.com/a\", parser))\n",
        "result = await crawler.crawl(\"http://site.com/a\", parser)\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(\"Visited URLs:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUxa14AV73-q",
        "outputId": "6d505a3f-4a01-4c39-cd92-45122ed40a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 0.03406953811645508 seconds\n",
            "Visited URLs: ['http://site.com/a', 'http://site.com/d', 'http://site.com/b', 'http://site.com/c']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HybridAsyncCrawler completes"
      ],
      "metadata": {
        "id": "3Fl-vd39_WLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AsyncCrawler:\n",
        "    def __init__(self):\n",
        "        self.visited = set()\n",
        "        self.lock = asyncio.Lock()\n",
        "        self.que = asyncio.Queue()\n",
        "\n",
        "    async def downloadUrls(self, url, htmlParser, hostname):\n",
        "        # next_urls = htmlParser.parse(url)  # assuming async getUrls ; here we don't have like that , we are not returning async result\n",
        "        next_urls = await asyncio.to_thread(htmlParser.parse, url)\n",
        "        for u in next_urls:\n",
        "            if urlparse(u).hostname == hostname:\n",
        "                async with self.lock:\n",
        "                    if u not in self.visited:\n",
        "                        self.visited.add(u)\n",
        "                        await self.que.put(u)\n",
        "\n",
        "    async def crawl(self, startUrl: str, htmlParser):\n",
        "        hostname = urlparse(startUrl).hostname\n",
        "        self.visited = {startUrl}\n",
        "        await self.que.put(startUrl)\n",
        "\n",
        "        tasks = []\n",
        "\n",
        "        while not self.que.empty() or tasks:\n",
        "            while not self.que.empty():\n",
        "                url = await self.que.get()\n",
        "                task = asyncio.create_task(self.downloadUrls(url, htmlParser, hostname))\n",
        "                tasks.append(task)\n",
        "\n",
        "            tasks = [t for t in tasks if not t.done()]\n",
        "            await asyncio.sleep(0.01)  # yield control to event loop\n",
        "\n",
        "        return list(self.visited)"
      ],
      "metadata": {
        "id": "qelXXvxC83rD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = htmlparser(test_graph_1)\n",
        "crawler = AsyncCrawler()\n",
        "st_time = time.time()\n",
        "# result = asyncio.run(crawler.crawl(\"http://site.com/a\", parser))\n",
        "result = await crawler.crawl(\"http://site.com/a\", parser)\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(\"Visited URLs:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oleY07B85oo",
        "outputId": "00cac595-b2c3-4a4f-898e-71114bbb28c0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 0.032602787017822266 seconds\n",
            "Visited URLs: ['http://site.com/d', 'http://site.com/a', 'http://site.com/b', 'http://site.com/c']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AsyncCrawler completes"
      ],
      "metadata": {
        "id": "CqGXty1Z_aVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WebCrawler With Retry Starts\n",
        "\n"
      ],
      "metadata": {
        "id": "tW0wdYp-7LzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# This is HtmlParser's API interface.\n",
        "# You should not implement it, or speculate about its implementation\n",
        "# \"\"\"\n",
        "#class HtmlParser(object):\n",
        "#    def getUrls(self, url):\n",
        "#        \"\"\"\n",
        "#        :type url: str\n",
        "#        :rtype List[str]\n",
        "#        \"\"\"\n",
        "\n",
        "import asyncio\n",
        "from urllib.parse import urlparse\n",
        "class Solution:\n",
        "    def crawl(self, startUrl: str, htmlParser: 'HtmlParser') -> List[str]:\n",
        "        return asyncio.run(self._crawl_async(startUrl,htmlParser))\n",
        "\n",
        "\n",
        "    async def _crawl_async(self,startUrl: str, htmlParser: 'HtmlParser'):\n",
        "        visited= set()\n",
        "        domain = urlparse(startUrl).hostname\n",
        "        que = asyncio.Queue()\n",
        "        await que.put((startUrl,0))\n",
        "\n",
        "        async def crawl(max_limit=3):\n",
        "            while True:\n",
        "                popUrl,attempt = await que.get()\n",
        "\n",
        "                if popUrl in visited and attempt==0:\n",
        "                    que.task_done()\n",
        "                    continue\n",
        "\n",
        "                visited.add(popUrl)\n",
        "\n",
        "                try:\n",
        "                    next_urls = await asyncio.to_thread(htmlParser.getUrls,popUrl)\n",
        "                except:\n",
        "                    print(f\"getUrls for {popUrl} fails for attempt {attempt}\")\n",
        "                    if (attempt==max_limit):\n",
        "                        raise Exception(f\"Failed to getUrls for {popUrl}\")\n",
        "                    else:\n",
        "                        await que.put((popUrl,attempt+1))\n",
        "                    que.task_done()\n",
        "                    return\n",
        "\n",
        "\n",
        "                for next_url in next_urls:\n",
        "                    if next_url not in visited and urlparse(next_url).hostname==domain:\n",
        "                        await que.put((next_url,0))\n",
        "\n",
        "                que.task_done()\n",
        "\n",
        "        workers = [asyncio.create_task(crawl()) for _ in range(2)]\n",
        "        await que.join()\n",
        "\n",
        "        for worker in workers:\n",
        "            worker.cancel()\n",
        "\n",
        "        return list(visited)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OPQ2gxAn7Y35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Async WebCrawler With Retry Ends"
      ],
      "metadata": {
        "id": "Usq3yuPy7kzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse\n",
        "import asyncio\n",
        "class AsyncWebCrawlerGold:\n",
        "  def __init__(self):\n",
        "    self.queue = asyncio.Queue() # thread safe\n",
        "    self.visited= set() # need lock\n",
        "    self.lock = asyncio.Lock()\n",
        "\n",
        "  async def crawl(self,url,htmlParser,concurrency=10):\n",
        "\n",
        "    hostname = urlparse(url).hostname\n",
        "    await self.queue.put(url) # increment count\n",
        "\n",
        "    async def worker():\n",
        "      '''\n",
        "         get the url async way\n",
        "          1. with lock add to visited\n",
        "          2. thread to fetchUrl in async way\n",
        "          3. loop for all fetchUrls and add it to queue if not visited\n",
        "      '''\n",
        "      while True:\n",
        "\n",
        "        try:\n",
        "          popUrl = await asyncio.wait_for(self.queue.get(),timeout=2)\n",
        "        except asyncio.TimeoutError:\n",
        "          return # fail to get the url out for the worker, it's done\n",
        "\n",
        "        async with self.lock:\n",
        "          if popUrl in self.visited:\n",
        "            self.queue.task_done() # decrement count\n",
        "            continue\n",
        "          self.visited.add(popUrl)\n",
        "\n",
        "\n",
        "        try:\n",
        "          next_urls = await asyncio.to_thread(htmlParser.parse,popUrl)\n",
        "        except Exception as e:\n",
        "          print(f\"Can retry it failed with exception {e}\")\n",
        "          self.queue.task_done()\n",
        "          continue\n",
        "\n",
        "        for next_url in next_urls:\n",
        "          if urlparse(next_url).hostname == hostname:\n",
        "            async with self.lock:\n",
        "              if next_url not in self.visited:\n",
        "                await self.queue.put(next_url)\n",
        "\n",
        "        print(f\"done scrapping {popUrl}\")\n",
        "        self.queue.task_done()\n",
        "\n",
        "\n",
        "    workers = [ asyncio.create_task(worker()) for _ in range(concurrency)]\n",
        "    await self.queue.join() # non blocking ; wait for all tasks to be done even thouugh tasks is added in que dymanically\n",
        "\n",
        "    for w in workers:\n",
        "      w.cancel()\n",
        "\n",
        "    return list(self.visited)"
      ],
      "metadata": {
        "id": "baXAkOHrvLj1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = htmlparser(test_graph_1)\n",
        "crawler = AsyncWebCrawlerGold()\n",
        "st_time = time.time()\n",
        "# result = asyncio.run(crawler.crawl(\"http://site.com/a\", parser))\n",
        "result = await crawler.crawl(\"http://site.com/a\", parser)\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(\"Visited URLs:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn1wW_HAvS5p",
        "outputId": "b260a071-fb90-48d8-937f-ac26c3314f72"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done scrapping http://site.com/a\n",
            "done scrapping http://site.com/b\n",
            "done scrapping http://site.com/c\n",
            "done scrapping http://site.com/d\n",
            "Time taken: 0.0022017955780029297 seconds\n",
            "Visited URLs: ['http://site.com/d', 'http://site.com/a', 'http://site.com/b', 'http://site.com/c']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse\n",
        "import asyncio\n",
        "\n",
        "class AsyncWebCrawlerGoldWithRetries:\n",
        "    def __init__(self):\n",
        "        self.queue = asyncio.Queue()\n",
        "        self.visited = set()\n",
        "        self.lock = asyncio.Lock()\n",
        "\n",
        "    async def crawl(self, url, htmlParser, concurrency=10, max_retries=3):\n",
        "        hostname = urlparse(url).hostname\n",
        "        await self.queue.put((url, 0))  # enqueue with retry_count = 0\n",
        "\n",
        "        async def worker():\n",
        "            while True:\n",
        "                try:\n",
        "                    (popUrl, retry_count) = await asyncio.wait_for(self.queue.get(), timeout=2)\n",
        "                except asyncio.TimeoutError:\n",
        "                    return\n",
        "\n",
        "                async with self.lock:\n",
        "                    if popUrl in self.visited:\n",
        "                        self.queue.task_done()\n",
        "                        continue\n",
        "                    self.visited.add(popUrl)\n",
        "\n",
        "                try:\n",
        "                    next_urls = await asyncio.to_thread(htmlParser.parse, popUrl)\n",
        "                except Exception as e:\n",
        "                    print(f\"Fetch failed for {popUrl} (attempt {retry_count + 1}): {e}\")\n",
        "                    if retry_count < max_retries:\n",
        "                        backoff = 0.5 * (2 ** retry_count)\n",
        "                        print(f\"Retrying {popUrl} after {backoff:.2f}s\")\n",
        "                        await asyncio.sleep(backoff)\n",
        "                        await self.queue.put((popUrl, retry_count + 1))\n",
        "                    else:\n",
        "                        print(f\"Giving up on {popUrl}\")\n",
        "                    self.queue.task_done()\n",
        "                    continue\n",
        "\n",
        "                for next_url in next_urls:\n",
        "                    if urlparse(next_url).hostname == hostname:\n",
        "                        async with self.lock:\n",
        "                            if next_url not in self.visited:\n",
        "                                await self.queue.put((next_url, 0))  # reset retry count\n",
        "\n",
        "                print(f\"done scraping {popUrl}\")\n",
        "                self.queue.task_done()\n",
        "\n",
        "        workers = [asyncio.create_task(worker()) for _ in range(concurrency)]\n",
        "        await self.queue.join()\n",
        "\n",
        "        for w in workers:\n",
        "            w.cancel()\n",
        "\n",
        "        return list(self.visited)\n"
      ],
      "metadata": {
        "id": "yh0bf2G_0HRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "class AsyncWebCrawlerGoldWithDepthAndRateLimit:\n",
        "    def __init__(self, max_concurrent_requests=10, max_depth=3):\n",
        "        self.queue = asyncio.Queue()\n",
        "        self.visited = set()\n",
        "        self.lock = asyncio.Lock()\n",
        "        self.rate_limit = asyncio.Semaphore(max_concurrent_requests)  # limit concurrent fetches\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    async def crawl(self, url, htmlParser, max_retries=3):\n",
        "        hostname = urlparse(url).hostname\n",
        "        await self.queue.put((url, 0, 0))  # (url, retry_count, depth)\n",
        "\n",
        "        async def worker():\n",
        "            while True:\n",
        "                try:\n",
        "                    popUrl, retry_count, depth = await asyncio.wait_for(self.queue.get(), timeout=2)\n",
        "                except asyncio.TimeoutError:\n",
        "                    return\n",
        "\n",
        "                async with self.lock:\n",
        "                    if popUrl in self.visited:\n",
        "                        self.queue.task_done()\n",
        "                        continue\n",
        "                    self.visited.add(popUrl)\n",
        "\n",
        "                try:\n",
        "                    async with self.rate_limit:\n",
        "                        next_urls = await asyncio.to_thread(htmlParser.parse, popUrl)\n",
        "                except Exception as e:\n",
        "                    print(f\"Fetch failed for {popUrl} (attempt {retry_count + 1}): {e}\")\n",
        "                    if retry_count < max_retries:\n",
        "                        backoff = 0.5 * (2 ** retry_count)\n",
        "                        print(f\"Retrying {popUrl} after {backoff:.2f}s\")\n",
        "                        await asyncio.sleep(backoff)\n",
        "                        await self.queue.put((popUrl, retry_count + 1, depth))\n",
        "                    else:\n",
        "                        print(f\"Giving up on {popUrl}\")\n",
        "                    self.queue.task_done()\n",
        "                    continue\n",
        "\n",
        "                for next_url in next_urls:\n",
        "                    if urlparse(next_url).hostname == hostname:\n",
        "                        async with self.lock:\n",
        "                            if next_url not in self.visited and depth + 1 <= self.max_depth:\n",
        "                                await self.queue.put((next_url, 0, depth + 1))\n",
        "\n",
        "                print(f\"done scraping {popUrl}\")\n",
        "                self.queue.task_done()\n",
        "\n",
        "        workers = [asyncio.create_task(worker()) for _ in range(self.rate_limit._value)]\n",
        "        await self.queue.join()\n",
        "\n",
        "        for w in workers:\n",
        "            w.cancel()\n",
        "\n",
        "        return list(self.visited)\n"
      ],
      "metadata": {
        "id": "HQY2npJG36Hr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = htmlparser(test_graph_1)\n",
        "crawler = AsyncWebCrawlerGoldWithDepthAndRateLimit()\n",
        "st_time = time.time()\n",
        "# result = asyncio.run(crawler.crawl(\"http://site.com/a\", parser))\n",
        "result = await crawler.crawl(\"http://site.com/a\", parser)\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(\"Visited URLs:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r867P1m33-F8",
        "outputId": "4c5e60dc-bc1f-463e-b8b8-534c72c0288c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done scraping http://site.com/a\n",
            "done scraping http://site.com/b\n",
            "done scraping http://site.com/c\n",
            "done scraping http://site.com/d\n",
            "Time taken: 0.0015628337860107422 seconds\n",
            "Visited URLs: ['http://site.com/d', 'http://site.com/a', 'http://site.com/b', 'http://site.com/c']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Problem 2: Thread-Safe In-Memory Rate Limiter\n",
        "\n",
        "import time\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "class RateLimiter:\n",
        "    def __init__(self, limit: int, window: int):\n",
        "        self.limit = limit\n",
        "        self.window = window\n",
        "        self.user_requests = defaultdict(deque)\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def allow_request(self, user_id: str) -> bool:\n",
        "        current_time = time.time()\n",
        "        with self.lock:\n",
        "            q = self.user_requests[user_id]\n",
        "            while q and q[0] <= current_time - self.window:\n",
        "                q.popleft()\n",
        "            if len(q) < self.limit:\n",
        "                q.append(current_time)\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "# ✅ Problem 3: Concurrent Log Aggregator\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "class LogAggregator:\n",
        "    def __init__(self):\n",
        "        self.logs_by_source = defaultdict(list)\n",
        "        self.global_logs = []\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def add_log(self, source: str, line: str) -> None:\n",
        "        with self.lock:\n",
        "            self.logs_by_source[source].append(line)\n",
        "            self.global_logs.append((time.time(), source, line))\n",
        "\n",
        "    def get_logs(self, source: Optional[str] = None) -> List[str]:\n",
        "        with self.lock:\n",
        "            if source:\n",
        "                return list(self.logs_by_source[source])\n",
        "            else:\n",
        "                return [f\"[{src}] {line}\" for _, src, line in sorted(self.global_logs)]\n"
      ],
      "metadata": {
        "id": "B6l2sT24A76K"
      }
    }
  ]
}